{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d03v22dXENlK",
        "outputId": "66abfdb8-bbf7-4d5d-b9da-d83d61b1a303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script userpath is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts activate-global-python-argcomplete, python-argcomplete-check-easy-install-script and register-python-argcomplete are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script pipx is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m/bin/bash: line 1: pipx: command not found\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,185 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,514 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,224 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,619 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,513 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,738 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Fetched 20.7 MB in 2s (8,578 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  python3-pip-whl python3-setuptools-whl\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip-whl python3-setuptools-whl python3.10-venv\n",
            "0 upgraded, 3 newly installed, 0 to remove and 59 not upgraded.\n",
            "Need to get 2,474 kB of archives.\n",
            "After this operation, 2,885 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip-whl all 22.0.2+dfsg-1ubuntu0.5 [1,680 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-setuptools-whl all 59.6.0-1.2ubuntu0.22.04.2 [788 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3.10-venv amd64 3.10.12-1~22.04.7 [5,718 B]\n",
            "Fetched 2,474 kB in 1s (3,505 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-pip-whl.\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-pip-whl_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip-whl (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Selecting previously unselected package python3-setuptools-whl.\n",
            "Preparing to unpack .../python3-setuptools-whl_59.6.0-1.2ubuntu0.22.04.2_all.deb ...\n",
            "Unpacking python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package python3.10-venv.\n",
            "Preparing to unpack .../python3.10-venv_3.10.12-1~22.04.7_amd64.deb ...\n",
            "Unpacking python3.10-venv (3.10.12-1~22.04.7) ...\n",
            "Setting up python3-setuptools-whl (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Setting up python3-pip-whl (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up python3.10-venv (3.10.12-1~22.04.7) ...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[K  installed package \u001b[1minsanely-fast-whisper\u001b[0m \u001b[1m0.0.15\u001b[0m, installed using Python 3.10.12\n",
            "  These apps are now globally available\n",
            "    - insanely-fast-whisper\n",
            "⚠️  Note: '/root/.local/bin' is not on your PATH environment variable. These apps will not be\n",
            "    globally accessible until your PATH is updated. Run `pipx ensurepath` to automatically add it,\n",
            "    or manually modify your PATH in your shell's config file (e.g. ~/.bashrc).\n",
            "done! ✨ 🌟 ✨\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Upgrade pip and install pipx\n",
        "!pip install --quiet --upgrade pip\n",
        "!pip install --quiet --user pipx\n",
        "!pipx ensurepath\n",
        "\n",
        "# Install system dependencies\n",
        "!sudo apt-get update && sudo apt-get install -y python3.10-venv ffmpeg\n",
        "\n",
        "# Install Python packages\n",
        "!pip install --quiet yt_dlp transformers torch faiss-cpu\n",
        "\n",
        "# Install insanely-fast-whisper\n",
        "!/root/.local/bin/pipx install git+https://github.com/Vaibhavs10/insanely-fast-whisper.git\n",
        "\n",
        "!sudo apt-get install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XBR5BrNEi2r",
        "outputId": "1ebee8d9-af2e-4ee4-b582-f3595684d229"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import subprocess\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from yt_dlp import YoutubeDL\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "# Check if a CUDA-compatible GPU is available\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Define the LyricsEmbedding class\n",
        "class LyricsEmbedding:\n",
        "    \"\"\"\n",
        "    A class for generating semantic embeddings of song lyrics using the Sentence Transformers framework.\n",
        "\n",
        "    This class uses the 'all-mpnet-base-v2' model, which is optimized for semantic similarity tasks.\n",
        "\n",
        "    Attributes:\n",
        "        model_name (str): Name of the pre-trained model to use. Defaults to 'sentence-transformers/all-mpnet-base-v2'\n",
        "        model: The loaded SentenceTransformer model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'sentence-transformers/all-mpnet-base-v2'):\n",
        "        \"\"\"\n",
        "        Initialize the LyricsEmbedding class with a specified model.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): The name of the sentence-transformer model to use.\n",
        "        \"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.model.to(DEVICE)\n",
        "\n",
        "    def embed(self, lyrics: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate embeddings for a single piece of lyrics.\n",
        "\n",
        "        The method uses torch.no_grad() for efficiency during inference and normalizes\n",
        "        the embeddings to unit length, which is crucial for cosine similarity comparisons\n",
        "        in the FAISS index.\n",
        "\n",
        "        Args:\n",
        "            lyrics (str): The input lyrics text to embed\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: A 768-dimensional normalized embedding vector as float32.\n",
        "                       float32 is used for compatibility with FAISS.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            embedding = self.model.encode(\n",
        "                lyrics,\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=True\n",
        "            )\n",
        "        return embedding.astype('float32')\n",
        "\n",
        "    def batch_embed(self, lyrics_list: List[str]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate embeddings for multiple pieces of lyrics efficiently.\n",
        "\n",
        "        Uses batching to process multiple lyrics simultaneously, which is significantly\n",
        "        faster than processing them individually, especially on GPU.\n",
        "\n",
        "        Args:\n",
        "            lyrics_list (List[str]): List of lyrics texts to embed\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: A matrix of shape (n_lyrics, 768) containing the embeddings,\n",
        "                       where each row is a normalized embedding vector.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            embeddings = self.model.encode(\n",
        "                lyrics_list,\n",
        "                batch_size=32,\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=True\n",
        "            )\n",
        "        return embeddings.astype('float32')\n",
        "\n",
        "# Clean lyrics\n",
        "def clean_lyrics(lyrics: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean and standardize lyrics text to improve matching accuracy.\n",
        "\n",
        "    This function applies several preprocessing steps:\n",
        "\n",
        "    1. Removes metadata annotations: Eliminates text within square brackets like\n",
        "       [Verse], [Chorus], [Producer], etc., as they don'tcontribute to the semantic\n",
        "       meaning of the lyrics.\n",
        "\n",
        "    2. Normalizes whitespace:\n",
        "       - Reduces multiple consecutive newlines to single newlines to preserve\n",
        "         some structure while eliminating excessive spacing\n",
        "       - Replaces all types of whitespace (tabs, multiple spaces) with single spaces\n",
        "         to ensure consistent formatting\n",
        "\n",
        "    Args:\n",
        "        lyrics (str): Raw lyrics text that may contain metadata, irregular spacing,\n",
        "                     and other formatting elements\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned lyrics with consistent formatting and without metadata annotations\n",
        "    \"\"\"\n",
        "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics)\n",
        "    lyrics = re.sub(r'(\\n\\s*\\n)+', '\\n', lyrics)\n",
        "    lyrics = re.sub(r'\\s+', ' ', lyrics)\n",
        "    return lyrics.strip()\n",
        "\n",
        "# Load and preprocess the lyrics dataset\n",
        "def load_and_preprocess_dataset(file_path: str, top_n: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load and preprocess a large lyrics dataset efficiently using chunking.\n",
        "\n",
        "    The function processes the dataset in chunks of 500,000 rows to handle\n",
        "    large files without loading everything into memory at once. It selects\n",
        "    the top_n songs by view count to create a manageable, high-quality subset.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file containing the lyrics dataset\n",
        "        top_n (int): Number of most viewed songs to keep\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed dataset with columns ['title', 'artist', 'lyrics']\n",
        "                     containing the top_n most viewed songs\n",
        "    \"\"\"\n",
        "    chunksize = 500000\n",
        "    top_views_df = pd.DataFrame()\n",
        "\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
        "        chunk['lyrics'] = chunk['lyrics'].fillna('').apply(clean_lyrics)\n",
        "        chunk_top = chunk.nlargest(top_n, 'views')\n",
        "        top_views_df = pd.concat([top_views_df, chunk_top])\n",
        "\n",
        "    top_views_df = top_views_df.nlargest(top_n, 'views')\n",
        "    top_views_df = top_views_df[['title', 'artist', 'lyrics']]\n",
        "    return top_views_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Create and save the FAISS index and metadata\n",
        "def create_and_save_index(\n",
        "    top_views_df: pd.DataFrame,\n",
        "    index_path: str = \"lyrics_index.faiss\",\n",
        "    metadata_path: str = \"lyrics_metadata.json\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Create and save a FAISS similarity index for lyrics along with metadata.\n",
        "\n",
        "    Uses FAISS IndexFlatIP (Inner Product) index which is optimized for cosine\n",
        "    similarity searches.\n",
        "\n",
        "    The function:\n",
        "    1. Generates embeddings for all lyrics using the transformer model\n",
        "    2. Creates a FAISS index from these embeddings\n",
        "    3. Saves song metadata (titles, artists) separately for retrieval\n",
        "\n",
        "    Args:\n",
        "        top_views_df (pd.DataFrame): DataFrame with columns ['title', 'artist', 'lyrics']\n",
        "        index_path (str): Path to save the FAISS index\n",
        "        metadata_path (str): Path to save the JSON metadata\n",
        "    \"\"\"\n",
        "    embedder = LyricsEmbedding()\n",
        "    embeddings = embedder.batch_embed(top_views_df['lyrics'].tolist())\n",
        "\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    metadata = {\n",
        "        'titles': top_views_df['title'].tolist(),\n",
        "        'artists': top_views_df['artist'].tolist(),\n",
        "    }\n",
        "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "# Download and transcribe audio from YouTube\n",
        "def get_lyrics_from_youtube_url(youtube_url: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Download a YouTube video's audio and transcribe it to obtain lyrics.\n",
        "\n",
        "    Uses a three-step process:\n",
        "    1. Downloads audio using yt-dlp\n",
        "    2. Transcribes using Insanely-Fast-Whisper\n",
        "    3. Processes the transcription output into clean text\n",
        "\n",
        "    Args:\n",
        "        youtube_url (str): URL of the YouTube music video\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: Transcribed lyrics if successful, None if any step fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'temp.%(ext)s',\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "            }],\n",
        "            'quiet': True\n",
        "        }\n",
        "\n",
        "        with YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([youtube_url])\n",
        "\n",
        "        if not os.path.exists('temp.mp3'):\n",
        "            raise FileNotFoundError(\"The MP3 file (temp.mp3) was not created successfully.\")\n",
        "\n",
        "        whisper_command = f'/root/.local/bin/insanely-fast-whisper --file-name temp.mp3'\n",
        "        subprocess.run(whisper_command, shell=True, text=True)\n",
        "\n",
        "        os.remove('temp.mp3')\n",
        "\n",
        "        try:\n",
        "            with open('output.json', 'r') as file:\n",
        "                data = json.load(file)\n",
        "\n",
        "            if \"chunks\" in data:\n",
        "                lyrics = \" \".join(chunk[\"text\"] for chunk in data[\"chunks\"])\n",
        "            else:\n",
        "                lyrics = data.get(\"text\", \"\")\n",
        "\n",
        "            with open('combined_lyrics.txt', 'w') as output_file:\n",
        "                output_file.write(lyrics)\n",
        "\n",
        "            return lyrics\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error parsing JSON: {e}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing URL: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main function to get covers\n",
        "def get_covers(youtube_url: str, k: int = 5, model_name: str = 'sentence-transformers/all-mpnet-base-v2') -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Find potential cover songs by comparing lyrics similarity between a YouTube video\n",
        "    and a database of songs.\n",
        "\n",
        "    The function performs these steps:\n",
        "    1. Extracts and transcribes lyrics from the YouTube video\n",
        "    2. Converts lyrics to embeddings using the transformer model\n",
        "    3. Performs similarity search against the FAISS index\n",
        "    4. Returns the k most similar songs with similarity scores\n",
        "\n",
        "    Args:\n",
        "        youtube_url (str): URL of the YouTube video to analyze\n",
        "        k (int): Number of similar songs to return (default: 5)\n",
        "        model_name (str): Name of the sentence transformer model to use\n",
        "                         (default uses all-mpnet-base-v2 for its strong performance\n",
        "                         on semantic similarity tasks)\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: List of k dictionaries containing:\n",
        "            - Title: Song title\n",
        "            - Artist: Artist name\n",
        "            - Score: Similarity score (0-100)\n",
        "            Sorted by score in descending order\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        embedder = LyricsEmbedding(model_name)\n",
        "\n",
        "        lyrics = get_lyrics_from_youtube_url(youtube_url)\n",
        "        if not lyrics:\n",
        "            raise ValueError(\"Failed to extract lyrics from YouTube video.\")\n",
        "\n",
        "        embedding = embedder.embed(lyrics)\n",
        "\n",
        "        try:\n",
        "            index = faiss.read_index(\"lyrics_index.faiss\")\n",
        "            with open('lyrics_metadata.json', 'r', encoding='utf-8') as f:\n",
        "                metadata = json.load(f)\n",
        "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "            raise RuntimeError(f\"Failed to load index or metadata: {str(e)}\")\n",
        "\n",
        "        similarities, indices = index.search(embedding.reshape(1, -1), k)\n",
        "\n",
        "        results = []\n",
        "        for similarity, idx in zip(similarities[0], indices[0]):\n",
        "            score = ((similarity + 1) / 2) * 100\n",
        "            results.append({\n",
        "                \"Title\": metadata['titles'][idx],\n",
        "                \"Artist\": metadata['artists'][idx],\n",
        "                \"Score\": round(score, 1)\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_covers: {str(e)}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "36bd1061086043e8aa78b8440865641c",
            "5dec3c95ac3241519d1f32459f9cfe74",
            "fc62d34f008445008de49f72c1a3d394",
            "f3c1c5d4c1644c8897d9fcc95526cc3c",
            "6541422ea8a64103b26c2a347db3d289",
            "a924b596669d4f1cacdc6416baeb9ec4",
            "23c2e870283845f58804691ebdd0fd5d",
            "9bbfc0169f7b4bbea774ed387d62d742",
            "d17a54ba9a4748aab72122064835138e",
            "972f4e67c19a495db2f525b3469fbe61",
            "6547c227bf6b4bfb824b333c6ef0db3c"
          ]
        },
        "id": "BeWXGF9-Ei0Y",
        "outputId": "ec921b6a-2e68-4aec-cc11-e1e8e8e548c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/carlosgdcj/genius-song-lyrics-with-language-information?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3.04G/3.04G [00:52<00:00, 62.5MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36bd1061086043e8aa78b8440865641c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5dec3c95ac3241519d1f32459f9cfe74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc62d34f008445008de49f72c1a3d394",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3c1c5d4c1644c8897d9fcc95526cc3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6541422ea8a64103b26c2a347db3d289",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a924b596669d4f1cacdc6416baeb9ec4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23c2e870283845f58804691ebdd0fd5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bbfc0169f7b4bbea774ed387d62d742",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d17a54ba9a4748aab72122064835138e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "972f4e67c19a495db2f525b3469fbe61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6547c227bf6b4bfb824b333c6ef0db3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess the dataset\n",
        "    dataset_directory = kagglehub.dataset_download(\"carlosgdcj/genius-song-lyrics-with-language-information\")\n",
        "    csv_file_path = os.path.join(dataset_directory, 'song_lyrics.csv')\n",
        "    top_views_df = load_and_preprocess_dataset(csv_file_path, top_n=1000)\n",
        "\n",
        "    # Create and save the index\n",
        "    create_and_save_index(top_views_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_80P5hgEivk",
        "outputId": "7ff8bfd7-bd93-4dd6-8ab7-d481f6cd8e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches:\n",
            "Shape of You by Ed Sheeran (Score: 97.5)\n",
            "Perfect by Ed Sheeran (Score: 83.8)\n",
            "Luis Fonsi  Daddy Yankee - Despacito Remix ft. Justin Bieber English Translation by Genius English Translations (Score: 83.7)\n",
            "Thinking Out Loud by Ed Sheeran (Score: 83.2)\n",
            "Perfect Duet by Ed Sheeran & Beyonc (Score: 83.0)\n"
          ]
        }
      ],
      "source": [
        "# Video 1\n",
        "youtube_url = 'https://www.youtube.com/watch?v=BDC8Jr-gp_4'\n",
        "k = 5\n",
        "covers = get_covers(youtube_url, k)\n",
        "print(\"Top matches:\")\n",
        "for cover in covers:\n",
        "  print(f\"{cover['Title']} by {cover['Artist']} (Score: {cover['Score']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk5A7P1nEitF",
        "outputId": "cf8846df-6672-44d8-a91a-5d0b3395841f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches:\n",
            "Believer by Imagine Dragons (Score: 97.8)\n",
            "BTS - Magic Shop English Translation by Genius English Translations (Score: 83.6)\n",
            "Bitch Dont Kill My Vibe by Kendrick Lamar (Score: 83.5)\n",
            "​my tears ricochet by Taylor Swift (Score: 83.4)\n",
            "I spoke to the devil in miami he said everything would be fine by XXXTENTACION (Score: 83.0)\n"
          ]
        }
      ],
      "source": [
        "# Video 2\n",
        "youtube_url = 'https://www.youtube.com/watch?v=W_97b97G5ds'\n",
        "covers = get_covers(youtube_url, k)\n",
        "print(\"Top matches:\")\n",
        "for cover in covers:\n",
        "  print(f\"{cover['Title']} by {cover['Artist']} (Score: {cover['Score']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlrF5BcRZ9YN",
        "outputId": "caae7179-c749-46b5-f68c-767aa8eade21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches:\n",
            "Rap God by Eminem (Score: 87.1)\n",
            "Duppy Freestyle by Drake (Score: 85.0)\n",
            "Homicide by Logic (Score: 85.0)\n",
            "Greatest by Eminem (Score: 84.6)\n",
            "N.Y. State of Mind by Nas (Score: 84.3)\n"
          ]
        }
      ],
      "source": [
        "# Video 3\n",
        "youtube_url = 'https://www.youtube.com/watch?v=L53MZzuE0QY'\n",
        "k = 5\n",
        "covers = get_covers(youtube_url, k)\n",
        "print(\"Top matches:\")\n",
        "for cover in covers:\n",
        "  print(f\"{cover['Title']} by {cover['Artist']} (Score: {cover['Score']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-t6C_CrZ9VR",
        "outputId": "b96160e3-5403-49f1-dc5d-ebeb1be3d0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches:\n",
            "Get Lucky by Daft Punk (Score: 93.7)\n",
            "Circles by Post Malone (Score: 77.4)\n",
            "All Star by Smash Mouth (Score: 76.5)\n",
            "The Greatest Show by Hugh Jackman, Keala Settle, Zac Efron, Zendaya & The Greatest Showman Ensemble (Score: 76.3)\n",
            "Rewrite the Stars by Zac Efron (Score: 76.2)\n"
          ]
        }
      ],
      "source": [
        "# Video 4\n",
        "youtube_url = 'https://www.youtube.com/watch?v=9vmrPrYJPqI'\n",
        "k = 5\n",
        "covers = get_covers(youtube_url, k)\n",
        "print(\"Top matches:\")\n",
        "for cover in covers:\n",
        "  print(f\"{cover['Title']} by {cover['Artist']} (Score: {cover['Score']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD4RwwJXZ9Sg",
        "outputId": "edfdfbfa-7a86-4abd-b18e-08da71a131c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches:\n",
            "Get Lucky by Daft Punk (Score: 86.1)\n",
            "MONEY LONG by kizaru (Score: 80.0)\n",
            "One Dance by Drake (Score: 76.7)\n",
            "All Star by Smash Mouth (Score: 76.5)\n",
            "​through the late night by Travis Scott (Score: 76.1)\n"
          ]
        }
      ],
      "source": [
        "# Video 5\n",
        "youtube_url = 'https://www.youtube.com/watch?v=R6ATpAr7rQU'\n",
        "k = 5\n",
        "covers = get_covers(youtube_url, k)\n",
        "print(\"Top matches:\")\n",
        "for cover in covers:\n",
        "  print(f\"{cover['Title']} by {cover['Artist']} (Score: {cover['Score']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1WNRCZPZ9P_",
        "outputId": "6defc8c8-9be2-49b6-8052-861f233b6310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches:\n",
            "Bohemian Rhapsody by Queen (Score: 93.6)\n",
            "Sing About Me Im Dying of Thirst by Kendrick Lamar (Score: 80.4)\n",
            "Ride by twenty one pilots (Score: 79.6)\n",
            "Pink  White by Frank Ocean (Score: 79.4)\n",
            "When Im Gone by Eminem (Score: 78.8)\n"
          ]
        }
      ],
      "source": [
        "# Video 6\n",
        "youtube_url = 'https://www.youtube.com/watch?v=RmtP8X4ZErs'\n",
        "covers = get_covers(youtube_url, k)\n",
        "print(\"Top matches:\")\n",
        "for cover in covers:\n",
        "  print(f\"{cover['Title']} by {cover['Artist']} (Score: {cover['Score']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyRmiIzmZ9NU",
        "outputId": "d58f87bf-c78f-4543-f01d-1d17e7e5fa2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches:\n",
            "The Hills by The Weeknd (Score: 92.8)\n",
            "Star Shopping by Lil Peep (Score: 86.6)\n",
            "PRBLMS by 6LACK (Score: 86.1)\n",
            "Bad Things by Machine Gun Kelly & Camila Cabello (Score: 85.9)\n",
            "I Feel It Coming by The Weeknd (Score: 85.6)\n"
          ]
        }
      ],
      "source": [
        "# Video 7\n",
        "youtube_url = 'https://www.youtube.com/watch?v=DfMnRP0pk3A'\n",
        "k = 5\n",
        "covers = get_covers(youtube_url, k)\n",
        "print(\"Top matches:\")\n",
        "for cover in covers:\n",
        "  print(f\"{cover['Title']} by {cover['Artist']} (Score: {cover['Score']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg7m0O7TZ9Kl",
        "outputId": "bcaae68f-eadc-41ce-cc34-e70318f42335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches:\n",
            "Amorfoda by Bad Bunny (Score: 83.5)\n",
            "Tuyo by Rodrigo Amarante (Score: 83.4)\n",
            "Despacito by Luis Fonsi (Score: 82.9)\n",
            "Tú No Metes Cabra by Bad Bunny (Score: 79.4)\n",
            "Mi Gente by J Balvin & Willy William (Score: 78.3)\n"
          ]
        }
      ],
      "source": [
        "# Video 8\n",
        "youtube_url = 'https://www.youtube.com/watch?v=1BVP72VrGQs'\n",
        "k = 5\n",
        "covers = get_covers(youtube_url, k)\n",
        "print(\"Top matches:\")\n",
        "for cover in covers:\n",
        "  print(f\"{cover['Title']} by {cover['Artist']} (Score: {cover['Score']})\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}